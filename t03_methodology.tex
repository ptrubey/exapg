$\bm{Z} = R\bm{V}$, where $\bm{V} \in \mathbb{S}_{\infty}^{d-1}$; 
$R \in \mathbb{R}_+$; $R$ and $\bm{V}$ are independent.  
$\bm{Y} = T_p(\bm{V})$ is the normalization of $\bm{V}$ onto 
$\mathbb{S}_p^{d-1}$.

\subsection{Target Distribution (gamma prior)}
\begin{equation}
    \begin{aligned}
        \bm{y}_i \mid \bm{\alpha}_i &\sim
        \mathcal{PG}_p\left(\bm{Y}\mid\bm{\alpha}_i,\bm{1}\right)\\
        \bm{\alpha}_i &\sim G\\
        G &\sim \mathcal{PY}\left(\eta, d, G_0\right)        
    \end{aligned}
    ~\hspace{1cm}
    \begin{aligned}
        G_0 &= {\textstyle\prod}_{\ell = 1}^{d}\mathcal{G}(\alpha_{\ell}\mid \xi_{\ell},\tau_{\ell})\\
        \xi_{\ell} &\sim \mathcal{G}(\xi\mid a, b)\\
        \tau_{\ell} &\sim \mathcal{G}(\tau\mid c, d)
    \end{aligned}
\end{equation}
If we expand the Pitman--Yor process using stick-breaking notation with truncation point $J$, we can write the above as
\begin{equation}
    \begin{aligned}
        \nu_j\mid\eta &\sim \mathcal{B}(1 - d, \eta + kd)\\
        \bm{\pi} &= \pi(\bm{\pi})\\
        \alpha_{j\ell} &\sim \mathcal{G}(\alpha\mid\xi_{\ell},\tau_{\ell})
    \end{aligned}
    ~\hspace{1cm}
    \begin{aligned}
        \xi_{\ell} &\sim \mathcal{G}(\xi \mid a, b)\\
        \tau_{\ell} &\sim \mathcal{G}(\tau\mid c, d)\\
        \bm{y}_i \mid \ldots &\sim {\textstyle\sum}_{j = 1}^Jw_i\mathcal{PG}(\bm{y}\mid\bm{\alpha}_j, \bm{1})
    \end{aligned}   
\end{equation}
where $\pi(\bm{\nu})$ is a function of $\bm{\nu}$ such that
\begin{equation*}
    \bm{\pi} = \pi(\bm{v}) = \begin{cases}
        \nu_j &\text{ for }j = 1\\
        \nu_j{\textstyle\prod}_{k < j}(1 - \nu_k) &\text{ for }j = 2,\ldots,J-1\\
        {\textstyle\prod}_{k < j}(1 - \nu_k) &\text{ for }j = J
    \end{cases}
\end{equation*}
Fitting the above model is accomplished using a variational approximation.  As a first pass, we attempt a simple surrogate posterior using a mean field representation.

\subsubsection{Gaussian Mean Field Variational Family surrogate posterior for Pitman-Yor mixture of projected gammas, gamma prior}
The mean field surrogate posterior creates an independent distribution for every parameter of the model, ideally with the appropriate transformation
for each parameter such that the support of the transformed variable is the real line.   Optimization through gradient ascent makes discrete parameters ineligible
for fitting through a variational approach, so we ascertain cluster weights globally rather than sampling the underlying cluster membership.
\begin{equation}
    \begin{aligned}
    \bm{\nu} &\sim {\textstyle\prod}_{j = 1}^J\text{Logit}\mathcal{N}(\nu_j\mid\mu_{\nu_j},\sigma^2_{\nu_j})\\
    \bm{\alpha} &\sim {\textstyle\prod}_{j = 1}^J{\textstyle\prod}_{\ell = 1}^D\mathcal{LN}(\alpha_{j\ell}\mid\mu_{\alpha_{j\ell}},\sigma^2_{\alpha_{j\ell}})
    \end{aligned}
    ~\hspace{1cm}
    \begin{aligned}
    \bm{\xi} &\sim{\textstyle\prod}_{\ell = 1}^D\mathcal{LN}(\xi_{\ell}\mid\mu_{\xi_{\ell}},\sigma^2_{\xi_{\ell}})\\
    \bm{\tau} &\sim{\textstyle\prod}_{\ell = 1}^D\mathcal{LN}(\tau_{\ell}\mid\mu_{\tau_{\ell}}, \sigma^2_{\tau_{\ell}})
    \end{aligned}
\end{equation}
We fit this surrogate posterior in \emph{tensorflow} using automatic differentiation to calculate the gradient, and 

\subsection{Variational Approximation with exact sampling}
This variational approximation makes use of the exact sampling of latent variables as detailed in \cite{Loaizamaya2022} to
sample cluster membership, and thus cluster weights, via their full conditional distributions.  Then, conditional on
the sampled cluster weights/memberships, a simpler variational approximation is available for cluster and prior parameters.

Under the stick-breaking representation, cluster membership is distributed categorically as
\begin{equation}
    \text{P}(\gamma_i = j) \propto \pi_j\mathcal{PG}(\bm{y}_i\mid\bm{\alpha}_j,\bm{1})
\end{equation}
with the appropriate normalizing constant.  Then, the raw cluster weight $\nu_j$ is distributed as
\begin{equation}
    \nu_j\mid\bm{\gamma} \sim \text{Beta}(\eta - d + n_j,1 + jd + \sum_{k > j}n_k)
\end{equation}
where $n_j = \sum_{i:\gamma_i = j}1$.  Then $\bm{\pi} = \pi(\bm{\nu})$.

Let $\theta = (\bm{\alpha},\bm{\xi},\bm{\tau})$ be a subset of parameters for which the distribution will be approximated via variational inference,
and $\phi = (\bm{\gamma},\bm{\nu})$ be the latent parameters that will be sampled via their full conditional distributions.  Then,
\begin{equation}
    \begin{aligned}
        f(\bm{y},\theta\mid\phi) &\propto \mathcal \prod_{j = 1}^J\left[\prod_{i:\gamma_i = j}\left[\mathcal{PG}(\bm{y}_i\mid\bm{\alpha}_j,\bm{1})\right]\times\prod_{\ell = 1}^d\mathcal{G}(\alpha_{j\ell}\mid\xi_{\ell},\tau_{\ell})\right] \\
        &\hspace{1cm}\times \left[\prod_{\ell = 1}^d\mathcal{G}(\xi_{\ell}\mid a, b)\mathcal{G}(\tau_{\ell}\mid c,d)\right]        
    \end{aligned}
\end{equation}

\cite{tran2021}

% EOF