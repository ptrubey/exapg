In this paper, we have continued work on the BNP mixture of projected gammas model for angular data 
    to include a variational approximation.  We have developed means of clustering, or associating 
    storm profiles, through their emergent cluster identification post-model fitting.  We have developed
    a novel and flexible regression model with support on the unit $p$-norm sphere.  

We explored application of said models to the angular distribution of multivariate extremes, at a much 
    greater scale than has been previously attempted in our knowledge.  In doing so, we have some 
    weaknesses of the model in its application at scale.  We discuss remedies here now.

\subsection{Proposed solutions}
It is clear that, for our intended purpose of multivariate extreme inference on a high-dimensionality
    problem, the variational inference method we have chosen is inadequate to the task.  In such a
    problem, a target model where the discrete cluster identifiers have been integrated out leads to too
    few emergent clusters, and that lack of granularity is detrimental to the model's ability to recover
    the generating distribution.  There are a few potential remedies that we could take.  First, following
    the example of \cite{Loaizamaya2022}, we might reintroduce the latent cluster identifiers via a 
    Gibbs-sampling step.  Also by reintroducing the latent radii, the likelihood of $\bm{\alpha}$ becomes
    separable by dimension, meaning a mean-field variational approximation would not lose much information.
    With that said, an optimal variational approximation can only do as well as a well-tuned MCMC fitted
    model.  As \cite{chandra2023} shows, and as we observed by the count of emergent clusters decreasing as 
    dimensionality increases, model fidelity in recovering the generating distribution faces more challenges
    than merely an inadequate model fitting method.  We should also consider other solutions.

One additional step we can consider is placing a restriction on the shape parameter in the projected
    gamma model, such that $\max_s \alpha_s \geq 1$.  It is the case, when considering the data, that
    nearly every observation falls close to an edge on some dimension.
    Recollecting the gamma distribution, if the shape 
    parameter is less than one, then we observe a spike in density approaching zero.  In a projected 
    gamma setting, that translates to high density towards the edges of a distribution 
    where the shape parameters are less than 1.  If all dimensions are so, then we arrive at a very 
    unstable distribution with all mass around the edges of the support, and little to no mass in the 
    center.  For a very high dimensional problem, a single cluster with all shape parameters
    approaching zero is the inevitable end result, and thus, in fitting the model, we learn nothing.
    One means of removing that particular outcome as a possibility would be to enforce a restriction 
    that at least one dimension have a shape parameter greater than 1. \makenote{Expand on effects.}
    We might consider some dimensions as \emph{active} or \emph{inactive} for a given observation
    depending on whether that observation falls near that dimension's edge.  We can consider this
    restriction as requiring that at least one dimension is active.

Building on this notion of active or inactive dimensions, another extension we might consider would be
    a \emph{zero-inflated}, or \emph{sparse}, projected gamma, where some dimensions of the projected 
    gamma are structurally zero.  With some process controlling the structure of the zero-inflation,
    we can separate inference on active dimensions from that on inactive dimensions.  In the current model
    with a product-of-gammas centering distribution on $\bm{\alpha}$, it is currently the case that a
    single gamma density has to cover cluster parameters for both active dimensions, away from zero, 
    and inactive dimensions, near zero, resulting in hyper-parameter estimates approaching zero for 
    all dimensions.  This is not optimal.

One of the remedies suggested in \cite{chandra2023} is, rather than clustering on distributional 
    parameters in the target high-dimensional space, to instead cluster on a lower-dimensional 
    representation of that space.  That is, a factor analysis model.  We have taken a slightly
    different tack in this paper by developing a regression model.  This is another low-dimensional
    representation of the high-dimensional space.  But, for the regression model we have prepared, 
    the number of emergent clusters grows with the number of dimensions.  In fact, during model fitting,
    the \num{950} location sample quickly consumed all available candidate cluster slots up to the
    truncation level. We believe this occurs because the proposed model formula is inadequate to handle
    information contained in the output surface, so it is being absorbed by the cluster representation.
    Adding additional variables---making the regression model more descriptive of the output 
    surface---will likely result in fewer emergent clusters being necessary, and a higher-fidelity model.

In brief, there is a great deal more work to be done.

% EOF