Our analysis of the SLOSH data's extremal dependence requires some background.  
    \makenote{more appropriate location?}  For 
    convenience, let's also take this opportunity to define some of the notation 
    that will be used throughout the paper.  Let $i = 1,\ldots,n$ iterate over 
    observations in data.  In this context, that refers to \emph{storms}.
    Let $s = 1,\ldots,S$  iterate over dimensions in data.  In this context, 
    $s$ refers to \emph{sites}, or locations.

\subsection{Extreme Value Theory\label{ref:evt}}
% \begin{comment}
%     \begin{itemize}
%         \item Formal introduction of extreme analysis
%         \begin{itemize}
%             \item Separation of angular and radial components
%         \end{itemize}
%     \end{itemize}
% \end{comment}
\makenote{first paragraph basically copy-pasted from ndpg.  Rephrase more?}
Extreme value theory uses the limited information available in a sample to
    characterize the tails of a distribution.  In the univariate case, asymptotic
    results provide a unique parametric limiting family for the maximum observation
    in a sample.  For data with such a limiting distribution, a common approach
    is to consider observations in excess of a high threshold, and model the
    excesses over that threshold under a generalized Pareto framework.  This
    approach is known as \emph{peaks over threshold} (PoT).  In the multivariate 
    case, the theory for PoT is well established (see, for example, 
    \cite{dehaan2006}), and it indicates the existence of a limiting distribution
    with no parametric representation.  This can pose a challenge for inference,
    but we are not without options.

The multivariate PoT model considered in this paper has been developed 
    in~\cite{trubey:pg}, based on a definition of the limiting distribution proposed
    proposed in~\cite{rootzen2018}.  
    Let $\bm{W} = (W_1,\ldots,W_d)$ be a $d$--dimensional random vector with 
    cumulative distribution $F$.  Assuming there exists a sequence of vectors
    $\bm{a}_n$, $\bm{b}_n$, and a $d$--variate distribution $G$ such that 
    $\lim\limits_{n\to\infty}F^n(\bm{a}_n\bm{w} + \bm{b}_n) = G(\bm{w})$, then
    $G$ is a $d$--variate generalized extreme value distribution.  Then,
    \begin{equation}
        \label{eqn:threshold}
        \lim\limits_{n\to\infty}\text{Pr}
            \left[\bm{a}_n^{-1}(\bm{W} - \bm{b}_n) 
                \leq \bm{w}\mid \bm{W}\not\leq \bm{b}_n\right]
        = \frac{\log G(\bm{w}\wedge \bm{0}) - \log G(\bm{w})}{\log G(\bm{0})}
        = H(\bm{w})
    \end{equation}
    where $H$ is the multivariate Pareto distribution.  \cite{rootzen2018}
    provides a number of stochastic representations of $H$, and in particular,
    Remark~1 justifies the representation given in \cite{ferreira2014} where,
    in the limit, we factorize $\bm{W} = R\bm{V}$ with $R$ and $\bm{V}$ independent.
    \makenote{I'm not very fond of this transition.  We standardize $\bm{W}$ to 
    $\bm{Z}$, then factorize $\bm{Z} = R\bm{V}$.  Separability holds true in both
    cases (i think), but in the former, $\bm{V}$ is still dependent on the marginal 
    distributional parameters.  Standardization allows those to be ignored.  Or
    sidelined.  We should mention standardization before factorization.}
    $R = \lVert \bm{W}\rVert_{\infty}$ is distributed as a standard Pareto random
    variable, and $\bm{V} = \bm{W} / \lVert \bm{W}\rVert_{\infty}$ is a random
    vector existing within $\mathbb{S}_{\infty}^{d-1}$, the positive orthant of
    the unit sphere under the $\mathcal{L}_{\infty}$ norm.  $R$ and $\bm{V}$
    respectively comprise the \emph{radial} and \emph{angular} components of $H$.
    As $R$ and $\bm{V}$ are independent,  the distribution of $\bm{V}$ is 
    effectively the dependence structure of $\bm{W}$.

Let the threshold $b_{qs} = \hat{F}_{s}^{-1}(1 - q)$, where $\hat{F}$ is
    the empirical cumulative distribution function for the $s$th component.
    Marginally, values exceeding the threshold $b_{qs}$ are assumed to follow
    a univariate generalized Pareto distribution, and are used to estimate the
    corresponding marginal scale and shape parameters $a_{s}$ and $\xi_{s}$
    respectively.  Setting $\bm{b}$ and having inferred $\bm{a}$, and $\bm{\xi}$, 
    we transform $\bm{w}\mid \bm{w}\not\leq \bm{b}$ to a standard multivariate 
    Pareto form via the transformation
    \begin{equation}
        \label{eqn:standardization}
        z_{is} = \left(1 + \xi_{s}\frac{w_{is} 
            - b_{s}}{a_{s}}\right)_{+}^{1 / \xi_{s}}
    \end{equation}
    where $(\cdot)_+$ indicates the positive parts function.  Let 
    $r_i = \lVert \bm{z}_i\rVert$, and $\bm{v}_i = \bm{z}_i / r_i$.  Due to 
    thresholding, $i$ ranges from 1 to $m\leq n$, and $r_i > 1$.  Recall that
    $R\in (1, \infty)$ will, by design, follow a standard Pareto distribution,
    the inferential task left to us is describing the distribution of the
    angular component $\bm{V}\in\mathbb{S}_{\infty}^{d-1}$.  

\subsection{Projected Gamma\label{ref:pg}}
% \begin{comment}
%     \begin{itemize}
%         \item Introduction of Projected gamma distribution
%         \begin{itemize}
%             \item Projected Gamma as the kernel density of a BNP mixture
%         \end{itemize}
%     \end{itemize}
% \end{comment}
A suitable distribution for $\bm{V}$ can be approximated by projecting a 
    distribution in $\mathbb{R}_+^d$ onto $\mathbb{S}_{p}^{d-1}$.  
    Recall the $\mathcal{L}_p$ norm 
    $\lVert \bm{x}\rVert_p = \left(\sum_{s = 1}^dx^p\right)^{1/p}$.  Then
    for $\bm{x}\in\mathbb{R}_+^d$, we define the transformation
    \begin{equation}
        \label{eqn:projection}
        T_p(\bm{x}) = \left(\lVert \bm{x}\rVert_p, 
            \frac{x_1}{\lVert \bm{x}\rVert_p},\ldots, 
                \frac{x_{d-1}}{\lVert \bm{x}\rVert_p}\right)
                =: (r,\bm{y})
    \end{equation}
    where $y = (y_1,\ldots,y_{d-1}) \in \mathbb{S}_{p}^{d-1}$, $r > 0$, and 
    $y_d = (1 - \sum_{s = 1}^{d-1}y_{s}^p)^{\frac{1}{p}}$.
    By transforming $\bm{x}$ to $(r,\bm{y})$ and integrating out $r$, we
    succeed in establishing a distribution for $\bm{y}$ on 
    $\mathbb{S}_{\infty}^{d-1}$
    The Jacobian of the transformation in Equation~\eqref{eqn:projection} is
    $r^{d-1}[Y_d + \sum_{s = 1}^{d-1}y_{s}^py_d^{1-p}]$.
    This Jacobian is well suited to a product of gammas density, where 
    $f(\bm{x}\mid\bm{\alpha},\bm{\beta}) = 
        \prod_{s = 1}^d\mathcal{G}(x_{s}\mid\alpha_{s},\beta_{s})$.
    Transforming $\bm{x}$ as described, $r$ can be integrated out in closed
    form, leaving the \emph{projected gamma} density for arbitrary $p > 0$.
    \[
        f(\bm{y}\mid\bm{\alpha},\bm{\beta}) = \prod_{s = 1}^d\left[
            \frac{\beta_{s}^{\alpha_{s}}}{\Gamma(\alpha_{s})}
            y_{s}^{\alpha_{s} - 1}\right]
            \left[y_d + \sum_{s = 1}^{d-1}y_{s}^py_d^{1-p}\right]
            \frac{\Gamma(\sum_{s = 1}^d \alpha_{s})}{\left(
                \sum_{s = 1}^d\beta_{s}y_{s}
                \right)^{\sum_{s = 1}^d \alpha_{s}}
            }
    \]
    Of course, at $p=\infty$, the transformation is not differentiable, so a 
    direct projection onto $\mathbb{S}_{\infty}^{d-1}$ is not possible. We
    instead desire a high but finite $p$.
    Here we balance two issues: as $p\to\infty$, the space $\mathbb{S}_{p}^{d-1}$ 
    will approach asymptotically towards $\mathbb{S}_{\infty}^{d-1}$.
    Unfortunately, also as $p\to\infty$, the stability of the Jacobian of the
    transformation becomes increasingly dependent on the value, or choice, of 
    $y_d$.  If $y_d\to 0$, then the Jacobian diverges, and the distribution 
    becomes numerically unstable. We select $p = 10$ to balance these concerns.

To more faithfully capture the structure of the data, we use the projected gamma density 
    as a kernel density of an Bayesian non-parametric mixture model, based on 
    the Pitman-Yor process introduced in~\cite{perman1992}.  A Pitman-Yor process
    is a fully atomic measure specified by two parameters and a centering
    distribution.  Under a stick-breaking representation as described 
    in~\cite{ishwaran2001}, 
    \begin{equation}
        \label{eqn:stickbreak}
        \text{Pr}(\bm{\alpha}\mid\ldots) 
            = \sum_{j = 1}^Jp_j\delta_{\bm{\alpha}_j};\;\;\;
            \sum_{j=1}^Jp_j = 1;\;\;\;
            p_j := \chi_j\prod_{k = 1}^{j-1}(1 - \chi_k)
    \end{equation}
    where $\delta_{\bm{\alpha}_j}$ indicates a point mass at $\bm{\alpha}_j$ and
    $\bm{\alpha}_j$ are sampled independently from the centering distribution $G_0$.
    Under the Pitman-Yor process, $\chi_j \sim \text{Beta}(1 - d, \eta + jd)$
    where $d \in [0, 1)$ denotes the \emph{discount} parameter and $\eta > -d$
    denotes the \emph{concentration} parameter.

For the rest of this paper, let $i$ denote indexing over storm simulations (observations), 
    specifically those which survived the thresholding necessitated by the assumption of 
    Equation~\eqref{eqn:threshold}.  Let $s \in \lbrace 1, \ldots, S\rbrace$ denote indexing 
    over locations t which the storm surge is simulated.  That is, $y_{is}$ is the maximum simulated
    storm surge at location $s$ during storm $i$, after projection onto $\mathbb{S}_p^{d-1}$.
    Let $j\in \lbrace 1,\ldots,J\rbrace$ denote indexing over \emph{clusters}, the aforementioned
    mixture components up to truncation point $J$.  Then, the model can be specified as
    \begin{equation}
        \begin{aligned}
            \bm{y}_i \mid \bm{\alpha}_i &\sim
                \mathcal{PG}_p\left(\bm{Y}\mid\bm{\alpha}_i,\bm{1}\right)\\
            \bm{\alpha}_i &\sim G\\
            G &\sim \mathcal{PY}\left(\eta, \zeta, G_0\right)        
        \end{aligned}
        ~\hspace{1cm}
        \begin{aligned}
            G_0 &= {\textstyle\prod}_{s = 1}^{S}\mathcal{G}(\alpha_{s}\mid \xi_{s},\tau_{s})\\
            \xi_{s} &\sim \mathcal{G}(\xi\mid a, b)\\
            \tau_{s} &\sim \mathcal{G}(\tau\mid c, d)
        \end{aligned} 
    \end{equation}
    where $\eta$ and $\zeta$ are respectively the concentration and discount parameters
    of the Pitman--Yor process.  Fitting this model for a moderate number of sites (up to 50)
    can be accomplished in reasonable time via Markov-chain Monte Carlo methods.
    We introduce a latent cluster assignment variable $\delta_i$ such that 
    $\text{P}(\delta_i = j\mid\bm{y},\bm{\alpha}) = 
        \text{P}(\bm{\alpha}_i = \bm{\alpha}_j\mid \bm{\alpha},\bm{y})$ as described in 
        Equation~\eqref{eqn:stickbreak}.
    Then the full conditional distribution of $\delta_i$ is
    \begin{equation}
        \label{eqn:pdelta}
        \text{P}\left(\delta_i = j\mid \bm{y},\bm{\alpha},\bm{\nu}\right) = 
            \frac{p_j\mathcal{PY}(\bm{y}_i\mid\bm{\alpha}_j)}{
                \sum_{k = 1}^J p_k\mathcal{PY}(\bm{y}_i\mid\bm{\alpha}_j)}
                \;\hspace{1cm}\;
            \nu_j\mid\bm{\delta} \sim \mathcal{B}\left(1 + n_j - d, \sum_{k = j+1}^J n_k + \eta + jd\right)
    \end{equation}
    where $p_j = \nu_j\prod_{k=1}^{j-1}(1 - \nu_k)$,  as described in in 
    Equation~\eqref{eqn:stickbreak}, and $n_j = \sum_{i = 1}^n\bm{1}_{\delta_i = j}$.  Posterior 
    updates for $\bm{\alpha}$ and hyperprior parameters follow through their familiar gamma-gamma 
    form.

Such a fitting scheme works well for a moderately sized inference problem.  In \cite{trubey:pg},
    they report taking approximately \num{20} minutes to run the sampler for \num{40000} iterations 
    on an inference task of \num{532} observations $\times$ \num{47} sites.  If we want to conduct
    MCMC inference for more sites, and more observations, the sampler would need more iterations to
    reach convergence, and each iteration would take comparatively longer.  We must consider an
    alternative method of model fitting.

\subsection{Variational Inference - A Brief Overview}
\makenote{These 2 paragraphs are intended to introduce the justification for 
    VarBayes. I like the wordsmithing at the end of the second paragraph, but 
    given the current position of this intro, they're unnecessary.  REVISE.}
Let $\bm{x}$ be the observed data, and $\bm{\theta}$ be an unobserved set of 
    parameters governing the distribution of the observed data.  We term 
    $f(\bm{x}\mid\bm{\theta})$ the likelihood.  The goal of inference in 
    general is to obtain information about $\theta$, using information 
    obtained from $\bm{x}$. Bayesian inference in particular places a prior
    distribution on $\theta$, $f(\bm{\theta})$, and from this we can obtain a
    posterior distribution on $\bm{\theta}$ that incorporates both information
    from the likelihood as well as the prior.  That is,
    \[
        f(\bm{\theta}\mid\bm{x}) = 
            \frac{f(\bm{x}\mid\bm{\theta})f(\bm{\theta})}{f(\bm{x})},
    \]
    where the demoninator $f(\bm{x})$ is obtained as
    \[
        f(\bm{x}) = \int_{\Omega_{\bm{\theta}}}f(\bm{x}\mid\bm{\theta})f(\bm{\theta})d\bm{\theta}
    \]
    where $\Omega_{\bm{\theta}}$ denotes the support of $\bm{\theta}$.

When a given model is sufficiently complex that the above integration is no
    longer feasible analytically, or if the posterior cannot be described in
    closed form\makenote{when the posterior is not tractible...}, we frequently 
    turn to sampling--based classes of methods, such as 
    \emph{Markov chain Monte Carlo} (MCMC)\needcite. There are myriad approaches, 
    \makenote{expand?}, but central to all sampling--based 
    methods is the stochastic nature of model fitting, and the 
    necessity of samples for posterior inference.  Sampling methods can be 
    tempermental, requiring many numbers of iterations to reach convergence.
    Samples take time to draw, and memory to store; but for sufficiently large 
    problems, both can be in short supply.

One class of methods that shows promise in avoiding the computational and 
    storage burdens of sampling methods is variational inference.\needcite 
    Based on variational calculus, this approach attempts to approximate the 
    true posterior distribution
    $f(\bm{\theta}\mid\bm{x})$ using a tractible \emph{surrogate posterior} 
    $q(\bm{\theta})$.  Model fitting under this class involves specification of 
    the family of surrogate densities $\mathcal{Q}$, then selecting that density 
    $q^*$ which minimizes the \emph{Kullbeck-Liebler} (KL) divergence between 
    the surrogate and true posterior.  That is,
    \begin{equation}
        \label{eqn:optimalq}
        q^*(\bm{\theta}) = \argmin_{q\in\mathcal{Q}}\left\lbrace
        \text{KL}\left(q(\bm{\theta})||f(\bm{\theta}\mid\bm{x})\right) 
        \coloneqq
        \text{E}_{q}\left[\log\left(
        \frac{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
        \right)\right]
        \right\rbrace.
    \end{equation}
    Using KL divergence as an analogue to distance, we are attempting to find
    the \emph{closest} tractable surrogate posterior $q$ within the family 
    $\mathcal{Q}$ to the true posterior.  
    As has been explained however, we do not necessarily have 
    $f(\bm{\theta}\mid\bm{y})$ directly.  Let 
    $f(\bm{y},\bm{\theta}) = f(\bm{y}\mid\bm{\theta})f(\bm{\theta})$.
    Then we reformulate the KL divergence into
    \[
        \begin{aligned}
        \text{KL}\infdiv{q(\bm{\theta}\mid\bm{\psi})}{f(\bm{\theta}\mid\bm{x})}
            &=\text{E}_{\bm{\psi}}\left[\log\left(
                \frac{q(\bm{\theta}\mid\bm{\psi})}{f(\bm{\theta}\mid\bm{x})}
                \right)\right] = \text{E}_{\bm{\psi}}\left[
                \log\left(\frac{q(\bm{\theta}\mid\bm{\psi})f(\bm{x})}{f(\bm{y},\bm{\theta})}
                \right)\right]\\
            &= \text{E}_{\bm{\psi}}\left[
                \log q(\bm{\theta}\mid\bm{\psi}) - \log f(\bm{x},\bm{\theta}) 
                + \log f(\bm{x})
                \right]\\
            &= \text{E}_{q}\left[
                \log q(\bm{\theta}) - \log f(\bm{x},\bm{\theta})\right] + 
                   \log f(\bm{x}).
        \end{aligned}
    \]
    Note that the second term, the \emph{evidence}, is constant with respect to 
    $\bm{\theta}$.  With a bit of rearrangement,
    \[
        \begin{aligned}
        \log f(\bm{x}) 
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
                - \text{E}_q\left[
                \log q(\bm{\theta}) - \log f(\bm{x},\bm{\theta})
                \right]\\
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
                + \mathcal{L}(\bm{\theta})
        \end{aligned}
    \]
    Thus maximizing $\mathcal{L}(\bm{\theta})$ is equvivalent to minimizing the 
    KL divergence. In this form, as KL divergence is a positive quantity, it is
    readily apparent that $\mathcal{L}(\bm{\theta})$ establishes a lower bound
    on the evidence.  As such, the variational literature has christened it the
    \emph{evidence lower bound}, often abbreviated to \emph{ELBO}.  As such,
    we can rewrite Equation~\eqref{eqn:optimalq} to
    \[
        q^*(\bm{\theta}) = \argmax_{q\in\mathcal{Q}} \mathcal{L}(\bm{\theta})
    \]
    Note that we specify $q$ so as to depend on a set of variational parameters 
    $\bm{\psi}$, such that choosing the optimal $\bm{q}$ means in effect choosing
    the optimal $\bm{\psi}$.  

Optimization of the ELBO with respect to $\bm{\psi}$ can be a daunting task.
    If $q$ is specified in such a way that samples of $\bm{\theta}$ can be 
    generated as a function of $\bm{\psi}$ and some independent random variable 
    $\epsilon$ such as $\psi_0\epsilon + \psi_1$ as in the case of a normal
    random variable, then given samples of $\bm{\theta}$ we can differentiate 
    the ELBO with respect to $\bm{\psi}$ to calculate the gradient of $\bm{\psi}$
    at the current value.  Using this gradient, we can iteratively move towards
    a better position---a more optimal $\bm{\psi}$.

That said, a path-based optimization is highly dependent upon the starting position,
    and if multiple local optima exist, there is no guarantee we will reach the
    true optimal value.  There are many approaches that seek to solve this issue,
    such as stochastic gradient descent, simulated annealing, \makenote{add additional algorithms}
    but the problem remains.  Even in our testing, using what is well regarded as
    one of the best optimization algorithms available, our results are still
    highly dependent upon starting position, both in terms of model fidelity, and
    in terms of the resulting number of extant mixture components.  That said, 
    another approach we can consider is finding a better starting position.

\subsection{Optimal starting position}

As we are considering a mixture model where potentially there exists a label switching
    issue, if the initializing distribution of mixture component parameters provides
    \emph{decent} coverage of the distribution of optimal parameters, then the most 
    important part of the initialization is the distribution of mixture \emph{weights}.  
    Here we consider three strategies.  First is, of course, random initialization.  
    Second is uniform initialization---initializing the variational parameters such that, 
    after transformation via stick-breaking, the expected probability of cluster assignment 
    is uniform among all clusters up to the truncation point. That is, for truncation 
    point $J$,
    \[
    \text{E}[\bm{\nu}] = \left(\frac{1}{J},\frac{1}{J-1},\ldots,\frac{1}{3},\frac{1}{2}\right)
    \]
    Finally, we can declare a rather optimistically named \emph{optimal} starting point 
    as a result of using an abridged MCMC sampler solely for cluster assignments, then after
    some number of iterations, declaring the full conditional distribution of the resultant 
    mixture weights and associated parameters as the starting point for the VB algorithm.  
    Recall Equation~\eqref{eqn:pdelta}. 
    In the variational distribution $q$, we approximate the posterior distribution of $\nu$ 
    using a $\text{Logit}\mathcal{N}(\mu,\sigma^2)$ density.  Following \cite{aitchison1980}, 
    the \emph{best} approximation to $\mathcal{B}(\alpha_1,\alpha_2)$ as measured by minimum 
    KL divergence is achieved by letting $\mu = \psi(\alpha_1) - \psi(\alpha_2)$, and 
    $\sigma^2 = \psi^{\prime}(\alpha_1) + \psi^{\prime}(\alpha_2)$, where 
    $\psi(\cdot)$ and $\psi^{\prime}(\cdot)$ represent the digamma and trigamma functions 
    respectively.  We take these values $\bm{\mu}$, $\bm{\sigma}$ as a starting position for
    the variational approximation.

\begin{figure}[htb]
    \caption{Rise in energy score over baseline by number of dimensions, on simulated data 
    for various model fitting strategies. The faceting denotes the number of mixture components 
    in the generating distribution. \label{fig:energyscore}}
    \includegraphics[width=\linewidth]{plots/energy_score}
\end{figure}

Figure~\ref{fig:energyscore} displays the results of our foray looking into this 
    optimal starting position, by examining the rise in energy score calculated from a
    posterior predictive sample over a baseline energy score calculated from
    another random sample from the same generating distribution.  We see that a pure 
    MCMC approach performs best, but using an abridged MCMC sampler to set a 
    starting position for the variational algorithm performs nearly as well.  It is
    also much faster \makenote{this needs to be quantified}.
    








\subsection{SLOSH}
\begin{comment}
    \begin{itemize}
        \item More in-depth discussion of dataset
        \begin{itemize}
            \item Break dataset down into 3 levels of analysis; t90, all--but--road--features, airports.
            \item Descriptive statistics thereof
            \item Pitfalls in analysis?
        \end{itemize}
    \end{itemize}
\end{comment}

The projected gamma based model is not an inherently spatial model.  
    \makenote{change reasoning such that we are interested in \emph{specific} locations
    rather than change from grid to specific locations being due to computational
    complexity.}
    As such, rather than holding the entire grid in memory, it makes sense to consider 
    data pertaining only to landmarks of interest.    If one is performing 
    contingency planning in preparation for the storm season, it would be 
    helpful to know the probability of such services being rendered inoperable 
    simultaneously, precisely when they are needed most.  If one is seeking 
    sites for a new service provider, it would be helpful understand the 
    likelihood of a proposed location being rendered inoperable in the same 
    manner.  For these reasons, and considering the computational complexity of 
    a 23 million dimensional model, rather than consider the entire grid of data 
    pertaining to any particular storm, we subset the data to grid cells in the 
    vicinity of such locations of interest.  These locations are gathered from 
    the 2023 US Census Bureau's \emph{TIGER} database \needcite and specifically 
    the point and landmark file.  If a grid cell falls within 70 meters of a location,
    then the grid cell is identified with the location.  If multiple grid cells
    are within 70 meters, then the nearer one is identified with the location.
    Additionally, we select locations, or grid cells, that have experienced at
    at least some inundation in some $q$ proportion of storm simulations.  That is,
    such that $b_{qs} > 0$, for all $s$ of interest.  This restriction arises
    as a consequence of fitting the parameters for the marginal generalized Paretos:
    if the threshold for excesses $b$ is not set above $0$, the likelihood can diverge, 
    and MLE estimation of the parameters suffers. Here we experience another
    tradeoff, the implications of which are explored in 
    Figure~\ref{fig:thresholdselection}.  Setting a higher threshold allows more 
    sites to be included in the analysis, but in turn reduces the number of storm 
    simulations which exceed the threshold in at least one dimension.  This in turn 
    reduces the amount of information available by which we can estimate the 
    dependence structure.  We create several \emph{slices} of the SLOSH data,
    summarized in Table~\ref{tab:datasets}.

\begin{figure}[ht]
    \centering
    \caption{Trade-offs in threshold specification:
    (Left) Proportion of sites with threshold $b_{qs} > 0$ versus $(1 - q)$; 
    (Right) Proportion of storms $\bm{W}_i \not< \bm{b}_q$ versus $(1-q)$.
    \label{fig:thresholdselection}}
    \includegraphics[width=0.9\linewidth]{plots/explore_threshold}
\end{figure}

\begin{table}[htb]
    \centering
    \caption{Slices of SLOSH.  The quantile columns list quantiles \emph{of the threshold}. \makenote{quantiles unnecessary... excise?}\label{tab:datasets}}
    \include{./tables/datadesc}
\end{table}

At this point, we consider the analysis of the SLOSH data in 3 parts.  First, we
    look at locations pertaining to critical services, or infrastructure, which if 
    rendered unable to operate could drastically increase the negative effects, or 
    loss of life, due to the storm.
    In this context, we are interested in the probability of simultaneous failure.
    For this reason, we look at the transportation and emergency services slices
    of the SLOSH data.
    Second, we look at the overall shape of the inundation field in oder to use
    the clustering action of the Pitman-Yor process to find \emph{emergent} clusters
    of storm characteristics.  That is, we look for groupings of storm characteristics 
    for storms that presented similarly.  For this purpose, we are interested in a large
    sample size, with a large number of locations.  We will be comparing the clustering
    that results from the Threshold.9 and Limited datasets. \makenote{add locality grouping?
    it has better justification than "limited".}
    Realistically, using MCMC methods for model fitting at this scale would be prohibitively 
    expensive, so for that reason we consider a variational approach.
    Finally, \makenote{briefly describe regression goals and why specific datasets.}

\begin{figure}[b]
    \centering 
    \caption{Histograms of characteristics for simulations 
        that survived thresholding in \emph{Threshold.9}.\label{fig:thetahistogram}}
    \includegraphics[width=0.9\linewidth]{plots/threshold_histogram}
\end{figure}

Figure~\ref{fig:thetahistogram} shows the marginal histograms of storm parameters, for 
    storms which survived thresholding in the \emph{Threshold.9} slice.  Storm parameters in 
    the originating dataset were sampled via Latin hypercube, so would appear marginally
    uniform.  The difference between marginal uniform, and the observed densities provides
    some indication of what characteristics are necessary for a storm to exceed the threshold.
    In the first case, for sea level rise, it is readily apparent that a higher sea level will
    make it easier for a storm to inundate larger swaths of land, or inundate locations to a
    greater degree.  So we expect to, and in fact do, see a higher probability of a storm exceeding
    the threshold, for a higher sea level rise.  Similarly, a lower minimum pressure in the storm's
    eye corresponds to a more intense storm.  This bears out in a lower minimum pressure having
    a higher percentage of exceeding the threshold.  The relationship to approach speed is 
    interesting in that it's almost linear.  Perhaps, the mechanism there lies in that a higher
    approach speed indicates more power behind the storm.  The spike in approach angle past 360
    degrees is interesting as well. 360 degrees indicates due North, thus approach angles 
    beyond 360 degrees indicate the storm is heading slightly northeast. As these approaches are
    on the eastern seaboard, this means a shallower approach angle relative to the
    land---perhaps offering a given storm more time to inundate larger swaths of land.

% EOF 