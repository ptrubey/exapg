\begin{comment}
    \begin{itemize}
        \item Formal introduction of extreme analysis
        \begin{itemize}
            \item Separation of angular and radial components
        \end{itemize}
        \item Introduction of Projected gamma distribution
        \item More in-depth discussion of dataset
        \begin{itemize}
            \item Break dataset down into 3 levels of analysis; t90, all--but--road--features, airports.
            \item Descriptive statistics thereof
        \end{itemize}
    \end{itemize}
\end{comment}
Our analysis of the SLOSH data's extremal dependence requires some background.
    \makenote{introduce the review section.  avoid naked sections.}

\subsection{Extreme Value Theory\label{ref:evt}}
\makenote{first paragraph basically copy-pasted from ndpg.  Rephrase more?}
Extreme value theory uses the limited information available in a sample to
    characterize the tails of a distribution.  In the univariate case, asymptotic
    results provide a unique parametric limiting family for the maximum observation
    in a sample.  For data with such a limiting distribution, a common approach
    is to consider observations in excess of a high threshold, and model the
    excesses over that threshold under a generalized Pareto framework.  This
    approach is known as \emph{peaks over threshold} (PoT).  In the multivariate 
    case, the theory for PoT is well established (see, for example, 
    \cite{dehaan2006}), and it indicates the existence of a limiting distribution
    with no parametric representation.  This can pose a challenge for inference,
    but we are not without options.

The multivariate PoT model considered in this paper has been developed 
    in~\cite{trubey:pg}, based on a definition of the limiting distribution proposed
    proposed in~\cite{rootzen2018}.  
    Let $\bm{W} = (W_1,\ldots,W_d)$ be a $d$--dimensional random vector with 
    cumulative distribution $F$.  Assuming there exists a sequence of vectors
    $\bm{a}_n$, $\bm{b}_n$, and a $d$--variate distribution $G$ such that 
    $\lim\limits_{n\to\infty}F^n(\bm{a}_n\bm{w} + \bm{b}_n) = G(\bm{w})$, then
    $G$ is a $d$--variate generalized extreme value distribution.  Then,
    \begin{equation}
        \label{eqn:threshold}
        \lim\limits_{n\to\infty}\text{Pr}
            \left[\bm{a}_n^{-1}(\bm{W} - \bm{b}_n) 
                \leq \bm{w}\mid \bm{W}\not\leq \bm{b}_n\right]
        = \frac{\log G(\bm{w}\wedge \bm{0}) - \log G(\bm{w})}{\log G(\bm{0})}
        = H(\bm{w})
    \end{equation}
    where $H$ is the multivariate Pareto distribution.  \cite{rootzen2018}
    provides a number of stochastic representations of $H$, and in particular,
    Remark~1 justifies the representation given in \cite{ferreira2014} where,
    in the limit, we factorize $\bm{W} = R\bm{V}$ with $R$ and $\bm{V}$ independent.
    \makenote{I'm not very fond of this transition.  We standardize $\bm{W}$ to 
    $\bm{Z}$, then factorize $\bm{Z} = R\bm{V}$.  Separability holds true in both
    cases (i think), but the former is still dependent on marginal distributional parameters.
    standardization allows those to be ignored.  We should mention standardization
    before factorization.}
    $R = \lVert \bm{W}\rVert_{\infty}$ is distributed as a standard Pareto random
    variable, and $\bm{V} = \bm{W} / \lVert \bm{W}\rVert_{\infty}$ is a random
    vector existing within $\mathbb{S}_{\infty}^{d-1}$, the positive orthant of
    the unit sphere under the $\mathcal{L}_{\infty}$ norm.  $R$ and $\bm{V}$
    respectively comprise the \emph{radial} and \emph{angular} components of $H$.
    As $R$ and $\bm{V}$ are independent,  the distribution of $\bm{V}$ is 
    effectively the dependence structure of $\bm{W}$.

Let the threshold $b_{q\ell} = \hat{F}_{\ell}^{-1}(1 - 1/q)$, where $\hat{F}$ is
    the empirical cumulative distribution function for the $\ell$th component.
    Marginally, values exceeding the threshold $b_{q\ell}$ are assumed to follow
    a univariate generalized Pareto distribution, and are used to estimate the
    corresponding marginal scale and shape parameters $a_{\ell}$ and $\xi_{\ell}$
    respectively.  Setting $\bm{b}$ and having inferred $\bm{a}$, and $\bm{\xi}$, 
    we transform $\bm{w}\mid \bm{w}\not\leq \bm{b}$ to a standard multivariate 
    Pareto form via the transformation
    \begin{equation}
        z_{i\ell} = \left(1 + \xi_{\ell}\frac{w_{i\ell} 
            - b_{\ell}}{a_{\ell}}\right)_{+}^{1 / \xi_{\ell}}
    \end{equation}
    where $(\cdot)_+$ indicates the positive parts function.  Let 
    $r_i = \lVert \bm{z}_i\rVert$, and $\bm{v}_i = \bm{z}_i / r_i$.  Due to 
    thresholding, $i$ ranges from 1 to $m\leq n$, and $r_i > 1$.  Recall that
    $R\in (1, \infty)$ will, by design, follow a standard Pareto distribution,
    the inferential task left to us is describing the distribution of the
    angular component $\bm{V}\in\mathbb{S}_{\infty}^{d-1}$.  

\subsection{Projected Gamma\label{ref:pg}}
A suitable distribution for $\bm{V}$ can be approximated by projecting a 
    distribution in $\mathbb{R}_+^d$ onto $\mathbb{S}_{p}^{d-1}$.  
    Recall that the $\mathcal{L}_p$ norm 
    $\lVert \bm{s}\rVert_p = \left(\sum_{\ell = 1}^d\right)^{1/p}$.  Then
    for $\bm{x}\in\mathbb{R}_+^d$, we define the transformation
    \begin{equation}
        \label{eqn:projection}
        T_p(\bm{x}) = \left(\lVert \bm{x}\rVert_p, 
            \frac{x_1}{\lVert \bm{x}\rVert_p},\ldots, 
                \frac{x_{d-1}}{\lVert \bm{x}\rVert_p}\right)
                =: (r,\bm{y})
    \end{equation}
    where $y = (y_1,\ldots,y_{d-1}) \in \mathbb{S}_{p}^{d-1}$, $r > 0$, and 
    $y_d = \left(1 - \sum_{\ell = 1}^{d-1}y_{\ell}^p\right)^{\frac{1}{p}}$.
    By transforming $\bm{x}$ to $(r,\bm{y})$ and integrating out $r$, we
    succeed in establishing a distribution for $\bm{y}$ on 
    $\mathbb{S}_{\infty}^{d-1}$
    The Jacobian of the transformation in Equation~\eqref{eqn:projection} is
    $r^{d-1}\left[Y_d + \sum_{\ell = 1}^{d-1}y_{\ell}^py_d^{1-p}\right]$.
    This Jacobian is well suited to a product of gammas density, where 
    $f(\bm{x}\mid\bm{\alpha},\bm{\beta}) = 
        \prod_{\ell = 1}^d\mathcal{G}(x_{\ell}\mid\alpha_{\ell},\beta_{\ell})$.
    Transforming $\bm{x}$ as described, $r$ can be integrated out in closed
    form, and we are left with the \emph{projected gamma} density for arbitrary 
    $p > 0$.
    \[
        f(\bm{y}\mid\bm{\alpha},\bm{\beta}) = \prod_{\ell = 1}^d\left[
            \frac{\beta_{\ell}^{\alpha_{\ell}}}{\Gamma(\alpha_{\ell})}
            y_{\ell}^{\alpha_{\ell} - 1}\right]
            \left[y_d + \sum_{\ell = 1}^{d-1}y_{\ell}^py_d^{1-p}\right]
            \frac{\Gamma(\sum_{\ell = 1}^d \alpha_{\ell})}{\left(
                \sum_{\ell = 1}^d\beta_{\ell}y_{\ell}
                \right)^{\sum_{\ell = 1}^d \alpha_{\ell}}
            }
    \]
    Of course, at $p=\infty$, the transformation is not differentiable, so a 
    direct projection onto $\mathbb{S}_{\infty}^{d-1}$ is not possible. We
    instead desire a high but finite $p$.
    Here we balance two issues: as $p\to\infty$, the space $\mathbb{S}_{p}^{d-1}$ 
    will approach asymptotically towards $\mathbb{S}_{\infty}^{d-1}$.
    Unfortunately, also as $p\to\infty$, the stability of the Jacobian of the
    transformation becomes increasingly dependent on the value, or choice, of 
    $y_d$.  If $y_d\to 0$, then the Jacobian diverges, and the distribution 
    becomes numerically unstable. We select $p = 10$ to balance these concerns.

To more faithfully capture the structure of the data, we use the projected gamma density 
    as a kernel density of an Bayesian non-parametric mixture model, based on 
    the Pitman-Yor process introduced in~\cite{perman1992}.  A Pitman-Yor process
    is a fully atomic measure specified by two parameters and a centering
    distribution.  Under a stick-breaking representation as described 
    in~\cite{ishwaran2001}, 
    \[
        \text{Pr}(\bm{\alpha}\mid\ldots) 
            = \sum_{j = 1}^Jp_j\delta_{\bm{\alpha}_j};\;\;\;
            \sum_{j=1}^Jp_j = 1;\;\;\;
            p_j := \chi_j\prod_{k = 1}^{j-1}(1 - \chi_k)
    \]
    where $\delta_{\bm{\alpha}_j}$ indicates a point mass at $\bm{\alpha}_j$ and
    $\bm{\alpha}_j$ are sampled independently from the centering distribution $G_0$.
    Under the Pitman-Yor process, $\chi_j \sim \text{Beta}(1 - d, \eta + jd)$
    where $d \in [0, 1)$ denotes the \emph{discount} parameter and $\eta > -d$
    denotes the \emph{concentration} parameter.

For the rest of this paper, let $i$ denote indexing over storm simulations (observations), 
    specifically those which survived the thresholding necessitated by the assumption of 
    Equation~\eqref{eqn:threshold}.  Let $s \in \lbrace 1, \ldots, S\rbrace$ denote indexing 
    over locations t which the storm surge is simulated.  That is, $y_{is}$ is the maximum simulated
    storm surge at location $s$ during storm $i$, after projection onto $\mathbb{S}_p^{d-1}$.
    Let $j\in \lbrace 1,\ldots,J\rbrace$ denote indexing over \emph{clusters}, the aforementioned
    mixture components up to truncation point $J$.  Then, the model can be specified as
    \begin{equation}
        \begin{aligned}
            \bm{y}_i \mid \bm{\alpha}_i &\sim
                \mathcal{PG}_p\left(\bm{Y}\mid\bm{\alpha}_i,\bm{1}\right)\\
            \bm{\alpha}_i &\sim G\\
            G &\sim \mathcal{PY}\left(\eta, \zeta, G_0\right)        
        \end{aligned}
        ~\hspace{1cm}
        \begin{aligned}
            G_0 &= {\textstyle\prod}_{s = 1}^{S}\mathcal{G}(\alpha_{s}\mid \xi_{s},\tau_{s})\\
            \xi_{s} &\sim \mathcal{G}(\xi\mid a, b)\\
            \tau_{s} &\sim \mathcal{G}(\tau\mid c, d)
        \end{aligned} 
    \end{equation}
    where $\eta$ and $\zeta$ are respectively the concentration and discount parameters
    of the Pitman--Yor process.  Fitting this model for a moderate number of sites can be 
    accomplished via Markov-chain Monte Carlo methods.  Using the collapsed Gibbs sampler,
    We introduce a latent cluster assignment variable $\delta_i$ for each storm $i$ which 
    indicates which cluster storm $i$ belongs to.  Then the posterior distribution of

\subsection{SLOSH}
The projected gamma based model is not an inherently spatial model.  As such, 
    rather than holding the entire grid in memory, it makes sense to consider 
    data pertaining only to landmarks of interest.    If one is performing 
    contingency planning in preparation for the storm season, it would be 
    helpful to know the probability of such services being rendered inoperable 
    simultaneously, precisely when they are needed most.  If one is seeking 
    sites for a new service provider, it would be helpful understand the 
    likelihood of a proposed location being rendered inoperable in the same 
    manner.  For these reasons, and considering the computational complexity of 
    a 23 million dimensional model, rather than consider the entire grid of data 
    pertaining to any particular storm, we subset the data to grid cells in the 
    vicinity of such locations of interest.  These locations are gathered from 
    the 2023 US Census Bureau's \emph{TIGER} database \needcite and specifically 
    the point and landmark file, and only grid cells that experienced \emph{some} 
    inundation in greater than 2 \makenote{(subject to change)} percent of storm 
    simulations.   This last requirement arises as consequence of fitting the 
    generalized Pareto: the threshold for excesses $b$ needs to be set above the 
    minimum observed value $(0)$ of the dataset.  
    \makenote{(idea: zero-inflated projected gamma?)}  \makenote{Probably not
    necessary...} \st{We identify grid cells in}
    \st{the vicinity of such locations by establishing a small buffer around said}
    \st{locations, and spatially intersecting the buffer with the grid.}









% EOF 