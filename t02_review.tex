Our analysis of the SLOSH data's extremal dependence requires some background.  
    \makenote{more appropriate location?}  For 
    convenience, let's also take this opportunity to define some of the notation 
    that will be used throughout the paper.  Let $i = 1,\ldots,n$ iterate over 
    observations in data.  In this context, that refers to \emph{storms}.
    Let $s = 1,\ldots,S$  iterate over dimensions in data.  In this context, 
    $s$ refers to \emph{sites}, or locations.

\subsection{Extreme Value Theory\label{ref:evt}}
\makenote{first paragraph basically copy-pasted from ndpg.  Rephrase more?}
Extreme value theory uses the limited information available in a sample to
    characterize the tails of a distribution.  In the univariate case, asymptotic
    results provide a unique parametric limiting family for the maximum observation
    in a sample.  For data with such a limiting distribution, a common approach
    is to consider observations in excess of a high threshold, and model the
    excesses over that threshold under a generalized Pareto framework.  This
    approach is known as \emph{peaks over threshold} (PoT).  In the multivariate 
    case, the theory for PoT is well established (see, for example, 
    \cite{dehaan2006}), and it indicates the existence of a limiting distribution
    with no parametric representation.  This can pose a challenge for inference,
    but we are not without options.

The multivariate PoT model considered in this paper has been developed 
    in~\cite{trubey:pg}, based on a definition of the limiting distribution proposed
    proposed in~\cite{rootzen2018}.  
    Let $\bm{W} = (W_1,\ldots,W_d)$ be a $d$--dimensional random vector with 
    cumulative distribution $F$.  Assuming there exists a sequence of vectors
    $\bm{a}_n$, $\bm{b}_n$, and a $d$--variate distribution $G$ such that 
    $\lim\limits_{n\to\infty}F^n(\bm{a}_n\bm{w} + \bm{b}_n) = G(\bm{w})$, then
    $G$ is a $d$--variate generalized extreme value distribution.  Then,
    \begin{equation}
        \label{eqn:threshold}
        \lim\limits_{n\to\infty}\text{Pr}
            \left[\bm{a}_n^{-1}(\bm{W} - \bm{b}_n) 
                \leq \bm{w}\mid \bm{W}\not\leq \bm{b}_n\right]
        = \frac{\log G(\bm{w}\wedge \bm{0}) - \log G(\bm{w})}{\log G(\bm{0})}
        = H(\bm{w})
    \end{equation}
    where $H$ is the multivariate Pareto distribution.  \cite{rootzen2018}
    provides a number of stochastic representations of $H$, and in particular,
    Remark~1 justifies the representation given in \cite{ferreira2014} where,
    in the limit, we factorize $\bm{W} = R\bm{V}$ with $R$ and $\bm{V}$ independent.
    \makenote{I'm not very fond of this transition.  We standardize $\bm{W}$ to 
    $\bm{Z}$, then factorize $\bm{Z} = R\bm{V}$.  Separability holds true in both
    cases (i think), but in the former, $\bm{V}$ is still dependent on the marginal 
    distributional parameters.  Standardization allows those to be ignored.  Or
    sidelined.  We should mention standardization before factorization.}
    $R = \lVert \bm{W}\rVert_{\infty}$ is distributed as a standard Pareto random
    variable, and $\bm{V} = \bm{W} / \lVert \bm{W}\rVert_{\infty}$ is a random
    vector existing within $\mathbb{S}_{\infty}^{d-1}$, the positive orthant of
    the unit sphere under the $\mathcal{L}_{\infty}$ norm.  $R$ and $\bm{V}$
    respectively comprise the \emph{radial} and \emph{angular} components of $H$.
    As $R$ and $\bm{V}$ are independent,  the distribution of $\bm{V}$ is 
    effectively the dependence structure of $\bm{W}$.

Let the threshold $b_{qs} = \hat{F}_{s}^{-1}(1 - q)$, where $\hat{F}$ is
    the empirical cumulative distribution function for the $s$th component.
    Marginally, values exceeding the threshold $b_{qs}$ are assumed to follow
    a univariate generalized Pareto distribution, and are used to estimate the
    corresponding marginal scale and shape parameters $a_{s}$ and $\xi_{s}$
    respectively.  Setting $\bm{b}$ and having inferred $\bm{a}$, and $\bm{\xi}$, 
    we transform $\bm{w}\mid \bm{w}\not\leq \bm{b}$ to a standard multivariate 
    Pareto form via the transformation
    \begin{equation}
        \label{eqn:standardization}
        z_{is} = \left(1 + \xi_{s}\frac{w_{is} 
            - b_{s}}{a_{s}}\right)_{+}^{1 / \xi_{s}}
    \end{equation}
    where $(\cdot)_+$ indicates the positive parts function.  Let 
    $r_i = \lVert \bm{z}_i\rVert$, and $\bm{v}_i = \bm{z}_i / r_i$.  Due to 
    thresholding, $i$ ranges from 1 to $m\leq n$, and $r_i > 1$.  Recall that
    $R\in (1, \infty)$ will, by design, follow a standard Pareto distribution,
    the inferential task left to us is describing the distribution of the
    angular component $\bm{V}\in\mathbb{S}_{\infty}^{d-1}$.  

\subsection{Projected Gamma\label{ref:pg}}
A suitable distribution for $\bm{V}$ can be approximated by projecting a 
    distribution in $\mathbb{R}_+^d$ onto $\mathbb{S}_{p}^{d-1}$.  
    Recall the $\mathcal{L}_p$ norm 
    $\lVert \bm{x}\rVert_p = \left(\sum_{s = 1}^dx^p\right)^{1/p}$.  Then
    for $\bm{x}\in\mathbb{R}_+^d$, we define the transformation
    \begin{equation}
        \label{eqn:projection}
        T_p(\bm{x}) = \left(\lVert \bm{x}\rVert_p, 
            \frac{x_1}{\lVert \bm{x}\rVert_p},\ldots, 
                \frac{x_{d-1}}{\lVert \bm{x}\rVert_p}\right)
                =: (r,\bm{y})
    \end{equation}
    where $y = (y_1,\ldots,y_{d-1}) \in \mathbb{S}_{p}^{d-1}$, $r > 0$, and 
    $y_d = (1 - \sum_{s = 1}^{d-1}y_{s}^p)^{\frac{1}{p}}$.
    By transforming $\bm{x}$ to $(r,\bm{y})$ and integrating out $r$, we
    succeed in establishing a distribution for $\bm{y}$ on 
    $\mathbb{S}_{\infty}^{d-1}$
    The Jacobian of the transformation in Equation~\eqref{eqn:projection} is
    $r^{d-1}[Y_d + \sum_{s = 1}^{d-1}y_{s}^py_d^{1-p}]$.
    This Jacobian is well suited to a product of gammas density, where 
    $f(\bm{x}\mid\bm{\alpha},\bm{\beta}) = 
        \prod_{s = 1}^d\mathcal{G}(x_{s}\mid\alpha_{s},\beta_{s})$.
    Transforming $\bm{x}$ as described, $r$ can be integrated out in closed
    form, leaving the \emph{projected gamma} density for arbitrary $p > 0$.
    \[
        f(\bm{y}\mid\bm{\alpha},\bm{\beta}) = \prod_{s = 1}^d\left[
            \frac{\beta_{s}^{\alpha_{s}}}{\Gamma(\alpha_{s})}
            y_{s}^{\alpha_{s} - 1}\right]
            \left[y_d + \sum_{s = 1}^{d-1}y_{s}^py_d^{1-p}\right]
            \frac{\Gamma(\sum_{s = 1}^d \alpha_{s})}{\left(
                \sum_{s = 1}^d\beta_{s}y_{s}
                \right)^{\sum_{s = 1}^d \alpha_{s}}
            }
    \]
    Of course, at $p=\infty$, the transformation is not differentiable, so a 
    direct projection onto $\mathbb{S}_{\infty}^{d-1}$ is not possible. We
    instead desire a high but finite $p$.
    Here we balance two issues: as $p\to\infty$, the space $\mathbb{S}_{p}^{d-1}$ 
    will approach asymptotically towards $\mathbb{S}_{\infty}^{d-1}$.
    Unfortunately, also as $p\to\infty$, the stability of the Jacobian of the
    transformation becomes increasingly dependent on the value, or choice, of 
    $y_d$.  If $y_d\to 0$, then the Jacobian diverges, and the distribution 
    becomes numerically unstable. We select $p = 10$ to balance these concerns.

To more faithfully capture the structure of the data, we use the projected gamma density 
    as a kernel density of an Bayesian non-parametric mixture model, based on 
    the Pitman-Yor process introduced in~\cite{perman1992}.  A Pitman-Yor process
    is a fully atomic measure specified by two parameters and a centering
    distribution.  Under a stick-breaking representation as described 
    in~\cite{ishwaran2001}, 
    \begin{equation}
        \label{eqn:stickbreak}
        \text{Pr}(\bm{\alpha}\mid\ldots) 
            = \sum_{j = 1}^Jp_j\delta_{\bm{\alpha}_j};\;\;\;
            \sum_{j=1}^Jp_j = 1;\;\;\;
            p_j := \chi_j\prod_{k = 1}^{j-1}(1 - \chi_k)
    \end{equation}
    where $\delta_{\bm{\alpha}_j}$ indicates a point mass at $\bm{\alpha}_j$ and
    $\bm{\alpha}_j$ are sampled independently from the centering distribution $G_0$.
    Under the Pitman-Yor process, $\chi_j \sim \text{Beta}(1 - d, \eta + jd)$
    where $d \in [0, 1)$ denotes the \emph{discount} parameter and $\eta > -d$
    denotes the \emph{concentration} parameter.

For the rest of this paper, let $i$ denote indexing over storm simulations (observations), 
    specifically those which survived the thresholding necessitated by the assumption of 
    Equation~\eqref{eqn:threshold}.  Let $s \in \lbrace 1, \ldots, S\rbrace$ denote indexing 
    over locations t which the storm surge is simulated.  That is, $y_{is}$ is the maximum simulated
    storm surge at location $s$ during storm $i$, after projection onto $\mathbb{S}_p^{d-1}$.
    Let $j\in \lbrace 1,\ldots,J\rbrace$ denote indexing over \emph{clusters}, the aforementioned
    mixture components up to truncation point $J$.  Then, the model can be specified as
    \begin{equation}
        \label{eqn:pypg}
        \begin{aligned}
            \bm{y}_i \mid \bm{\alpha}_i &\sim
                \mathcal{PG}_p\left(\bm{Y}\mid\bm{\alpha}_i,\bm{1}\right)\\
            \bm{\alpha}_i &\sim G\\
            G &\sim \mathcal{PY}\left(\eta, \zeta, G_0\right)        
        \end{aligned}
        ~\hspace{1cm}
        \begin{aligned}
            G_0 &= {\textstyle\prod}_{s = 1}^{S}\mathcal{G}(\alpha_{s}\mid \xi_{s},\tau_{s})\\
            \xi_{s} &\sim \mathcal{G}(\xi\mid a, b)\\
            \tau_{s} &\sim \mathcal{G}(\tau\mid c, d)
        \end{aligned} 
    \end{equation}
    where $\eta$ and $\zeta$ are respectively the concentration and discount parameters
    of the Pitman--Yor process.  Fitting this model can be accomplished via Markov-chain 
    Monte Carlo methods. We can introduce a latent cluster assignment variable $\delta_i$ 
    such that 
    $\text{P}\left(\delta_i = j\mid\bm{y},\bm{\alpha}\right) = 
        \text{P}\left(\bm{\alpha}_i = \bm{\alpha}_j\mid \bm{y},\bm{\alpha}\right)$ as described in 
        Equation~\eqref{eqn:stickbreak}.
    Then the full conditional distribution of $\delta_i$ is
    \begin{equation}
        \label{eqn:pdelta}
        \text{P}\left(\delta_i = j\mid \bm{y},\bm{\alpha},\bm{\nu}\right) = 
            \frac{p_j\mathcal{PY}(\bm{y}_i\mid\bm{\alpha}_j)}{
                \sum_{k = 1}^J p_k\mathcal{PY}(\bm{y}_i\mid\bm{\alpha}_j)}
                \;\hspace{0.5cm}\;
            \nu_j\mid\bm{\delta} \sim \mathcal{B}\left(1 + n_j - d,\; 
                            {\textstyle \sum}_{k = j+1}^J n_k + \eta + jd\right)
    \end{equation}
    where $p_j = \nu_j\prod_{k=1}^{j-1}(1 - \nu_k)$,  as described in in 
    Equation~\eqref{eqn:stickbreak}, and $n_j = \sum_{i = 1}^n\bm{1}_{\delta_i = j}$.
    We can further introduce a latent $r_i\mid \bm{y}, \delta_i,\bm{\alpha} \sim 
        \mathcal{G}\left(r\mid \sum_s \alpha{\delta_is},\sum_s y_{is}\right)$ such that the
    likelihood of $\bm{\alpha}_j$ becomes separable by dimension.  Posterior updates 
    for $\bm{\alpha}$, and $\bm{\xi}$ can be accomplished via Metropolis-within-Gibbs steps,
    and the full conditional of $\tau_s$ has a known form as a Gamma distribution.

Such a fitting scheme can work well for a moderately sized inference problem.  In \cite{trubey:pg},
    they report requiring approximately \num{20} minutes to run the sampler for \num{40000} iterations, 
    on an inference task of \num{532} observations $\times$ \num{47} sites.  If we want to conduct
    MCMC inference for more sites, and more observations, the sampler may need more iterations to
    reach convergence, each iteration will require more cpu time, and the sampler require more 
    memory resources to hold the sampling history.  We consider an alternative method of model fitting.

\subsection{Variational Inference - A Brief Overview}
Variational inference, or variational Bayesian statistics, is an alternative method of 
    model-fitting that proposes, if the target distribution is analytically intractable,
    to fit a tractable candidate distribution as \emph{close} to the target distribution
    as possible.  That is, for data $\bm{x}$ and some distributional parameter set $\bm{\theta}$,
    where $f(\bm{\theta}\mid \bm{x})$ is not available in closed form, then we select
    \begin{equation}
        \label{eqn:optimalq}
        q^*(\bm{\theta}) = \argmin_{q\in\mathcal{Q}}\left\lbrace
        \text{KL}\left(q(\bm{\theta})||f(\bm{\theta}\mid\bm{x})\right) 
        \coloneqq
        \text{E}_{q}\left[\log\left(
        \frac{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
        \right)\right]
        \right\rbrace.
    \end{equation}
    There are myriad flavors of variational approaches, differentiating by
    the type and degree of structure, or dependence, they allow between the parameters in
    the variational distribution, or in the specific method they use to fit the variational
    distribution.  Of particular note here is \emph{mean field} variational Bayes, which 
    holds each parameter's variational distribution as independent of that from all other 
    parameters in the model.  That is,
    \[
        q_{\bm{\theta}} = \prod_{\ell \in L}q_{\theta_{\ell}}(\theta_{\ell}\mid
    \]
    If we hold each $q_{\theta_{\ell}}$ to be an appropriate transformation of a normal
    distribution, then each $q_{\theta_{\ell}}$ can be specified by a mean and variance 
    parameter. Some \needcite more complicated flavors of variational Bayes will relax 
    this independence restriction.

To expand, using KL divergence as an analogue to distance, variational inference attempting 
    to find the \emph{closest} tractable surrogate posterior $q$ within the family 
    $\mathcal{Q}$ to the true posterior.  
    As has been explained however, we do not necessarily have 
    $f(\bm{\theta}\mid\bm{y})$ directly.  Let 
    $f(\bm{y},\bm{\theta}) = f(\bm{y}\mid\bm{\theta})f(\bm{\theta})$.
    Then we reformulate the KL divergence into
    \[
        \begin{aligned}
        \text{KL}\infdiv{q(\bm{\theta}\mid\bm{\psi})}{f(\bm{\theta}\mid\bm{x})}
            &=\text{E}_{\bm{\psi}}\left[\log\left(
                \frac{q(\bm{\theta}\mid\bm{\psi})}{f(\bm{\theta}\mid\bm{x})}
                \right)\right] = \text{E}_{\bm{\psi}}\left[
                \log\left(\frac{q(\bm{\theta}\mid\bm{\psi})f(\bm{x})}{f(\bm{y},\bm{\theta})}
                \right)\right]\\
            &= \text{E}_{\bm{\psi}}\left[
                \log q(\bm{\theta}\mid\bm{\psi}) - \log f(\bm{x},\bm{\theta}) 
                + \log f(\bm{x})
                \right]\\
            &= \text{E}_{q}\left[
                \log q(\bm{\theta}) - \log f(\bm{x},\bm{\theta})\right] + 
                   \log f(\bm{x}).
        \end{aligned}
    \]
    Note that the second term, the \emph{evidence}, is constant with respect to 
    $\bm{\theta}$.  With a bit of rearrangement,
    \[
        \begin{aligned}
        \log f(\bm{x}) 
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
                - \text{E}_q\left[
                \log q(\bm{\theta}) - \log f(\bm{x},\bm{\theta})
                \right]\\
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{x})}
                + \mathcal{L}(\bm{\theta})
        \end{aligned}
    \]
    Thus maximizing $\mathcal{L}(\bm{\theta})$ is equivalent to minimizing the 
    KL divergence. In this form, as KL divergence is a positive quantity, it is
    readily apparent that $\mathcal{L}(\bm{\theta})$ establishes a lower bound
    on the evidence, and maximizing said lower bound is equivalent to minimizing
    the KL divergence.  Thus the variational literature has christened it the
    \emph{evidence lower bound}, often abbreviated to \emph{ELBO}.  As such,
    we can rewrite Equation~\eqref{eqn:optimalq} to
    \[
        q^*(\bm{\theta}) = \argmax_{q\in\mathcal{Q}} \mathcal{L}(\bm{\theta})
    \]
    Note that we specify $q$ so as to depend on a set of variational parameters 
    $\bm{\psi}$, such that choosing the optimal $\bm{q}$ means in effect choosing
    the optimal $\bm{\psi}$.  

Optimization of the ELBO with respect to $\bm{\psi}$ can be accomplished by
    analytically computing the gradient,
    \begin{equation}
        \label{eqn:gradient}
        \Delta_{\bm{\psi}} = \frac{\partial}{\partial \psi_{\ell}}
            \left\lbrace\text{E}_{q}\left[\log f(\bm{y}\mid\bm{\theta})\right] 
                - H(\bm{\psi})\right\rbrace
            \;\;\text{ for }\ell = 1,\ldots,d,
    \end{equation}
    where $H(\bm{\psi})$ is the entropy of $q$, and subject to some conditions\makenote{rephrase},
    locating the optimal point where $\Delta_{\bm{psi}} = \bm{0}$.  However, when 
    $\text{E}[\log f(\bm{x},\bm{\theta})]$ can not be analytically calculated, then
    one approach we can consider is the so-called \emph{reparameterization trick}.
    If $q$ is specified in such a way that samples of $\bm{\theta}$ can be generated
    as an invertible function $T$ of $\bm{\psi}$ and some independent random variable 
    $\epsilon$, then the differentiation in Equation~\ref{eqn:gradient} can be 
    moved inside the expectation,
    \begin{equation}
        \label{eqn:reparameterization}
        \Delta_{\bm{\psi}} = \text{E}\left[\frac{\partial}{\partial \psi_\ell}
            \log f\left(\bm{y},T(\bm{\epsilon},\bm{\psi})\right) -
            q(\theta\mid\psi)\right].
    \end{equation}
    This expectation is evaluated numerically, yielding the gradient at the current
    location. we can then iteratively move towards a better position---a more optimal 
    $\bm{\psi}$.  The variational literature is emphatic that for each optimization step,
    a single sample of $\bm{\epsilon}$ in Equation~\ref{eqn:reparameterization} is sufficient
    to provide convergence towards optimality.\makenote{[Citation needed]}

That said, a path-based optimization approach will be dependent upon the starting position,
    and if multiple local optima exist, there is no guarantee of reaching the
    true optimal value.  There are many approaches that seek to ameliorate this issue,
    such as stochastic gradient descent, simulated annealing, \makenote{add additional algorithms}
    In our analysis, we use the well-regarded \emph{Adam} optimizer \citep{kingma2017}, a combination of
    \emph{ADAGRAD} \citep{duchi2011} and \emph{RMSProp} \citep
    but in our analysis results are highly dependent upon starting position, both in terms of 
    model fidelity, and resulting number of extant mixture components.  One potential
    solution would be to consider a better starting position for the optimizer.

\subsection{Optimal starting position}

\makenote{Having excised the overarching methodology section, I'm unsure of where to place this 
    whole discussion. There's nothing really \emph{novel} in it; it only serves to inform the
    particular method we use for \emph{how} we do the variational Bayes model.  That said, I 
    don't know if it belongs in a "background and review" section.
    }

As we are considering a mixture model where potentially there exists a label switching
    issue, if the initializing distribution of mixture component parameters provides
    \emph{decent} coverage of the distribution of optimal parameters, then the most 
    important part of the initialization is the distribution of mixture \emph{weights}.  
    Here we consider three strategies.  First is random initialization, 
    \makenote{checking now if initialization at prior parameters will yield better result}.
    (\emph{VB Random}).  Second is uniform initialization---initializing the variational 
    parameters such that, after transformation via stick-breaking, the expected probability 
    of cluster assignment is uniform among all clusters up to the truncation point. That 
    is, for truncation point $J$,
    \[
        \text{E}[\bm{\nu}] = \left(\frac{1}{J},\frac{1}{J-1},\ldots,\frac{1}{3},\frac{1}{2}\right)
    \]
    leading to $\text{E}[p_j] = \frac{1}{J}$ for all $j = 1,\ldots,J$.
    Finally, we consider two variations of \emph{pregaming} 
    \makenote{I \emph{really} need a better name for this} 
    the variational algorithm, by setting the starting position using the results of an 
    abridged MCMC sampler.  
    Recall Equation~\eqref{eqn:pdelta}. In the variational distribution $q$, we approximate the 
    posterior distribution of $\nu$ using a $\text{Logit}\mathcal{N}(\mu,\sigma^2)$ density.  
    Following \cite{aitchison1980}, the \emph{best} approximation to 
    $\mathcal{B}(\alpha_1,\alpha_2)$ as measured by minimum KL divergence is achieved by 
    letting $\mu = \psi(\alpha_1) - \psi(\alpha_2)$, and 
    $\sigma^2 = \psi^{\prime}(\alpha_1) + \psi^{\prime}(\alpha_2)$, where 
    $\psi(\cdot)$ and $\psi^{\prime}(\cdot)$ represent the digamma and trigamma functions 
    respectively.  We take these values $\bm{\mu}$, $\bm{\sigma^2}$ as the starting position for
    the variational approximation. In the first method, \emph{VB Pregamed 1}, 
    we update $\bm{\nu}$, sampling new $\bm{\alpha}_j$'s using the initial setting
    for the centering distribution.  In the second (\emph{VB Pregamed 2}), we update the
    the parameters for the centering distribution as well. 
    \makenote{I should probably remove the first approach?  It clutters results needlessly.}

\begin{figure}[htb]
    \caption{Rise in energy score over baseline by number of dimensions, on simulated data 
    for various model fitting strategies. Faceting denotes the number of mixture components 
    in the generating distribution. \label{fig:energyscore}}
    \includegraphics[width=\linewidth]{plots/energy_score}
\end{figure}

To validate the variational model, and investigate the effects of different classes of 
    starting positions, we conduct a simulation study to evaluate various approaches for
    the BNP mixture of projected gammas model.  The datasets are simulated from a finite
    mixture of projected gammas, at varying levels of dimensionality and number of mixture
    components.  For each number of mixture components and dimensionality, 10 sets of
    parameters are generated, and then from for each parameter set, a training dataset and 
    testing dataset, each of \num{1000} replicates, are generated.

As the metric for our evaluation, we use the \emph{energy score} criterion \citep{gneiting2007}
    which is a generalization of the \emph{continuous ranked probability score} to a multivariate
    setting.  This energy score takes the form
    \[
      S_{\text{ES}}(P, \bm{x}) = \text{E}_p\left[g(\bm{X},\bm{x})\right] - 
        \frac{1}{2}\text{E}_p\left[g(\bm{X},\bm{X}^{\prime})\right]
    \]
    where $g$ is a kernel function, $\bm{x}$ is an observed value, and 
    $\bm{X},\bm{X}^{\prime}$ are posterior predictive replicates of $\bm{x}$.
    When energy score is used with an appropriate negative definite kernel 
    metric, it forms a \emph{proper} scoring rule. The specific negative 
    definite kernel we use is described in Prop.~3 of \cite{trubey:pg}.  Though 
    geodesic distance on $\mathbb{S}_{\infty}^{d-1}$ is computationally inefficient,
    for points on two different faces of $\mathbb{S}_{\infty}^{d-1}$,
    they find a computationally efficient upper bound on geodesic distance by 
    remembering that all faces are pairwise adjacent, and rotating one face into the
    same hyperplane as the other.  Then the kernel metric is the Euclidean distance
    between the first point, and the rotated second point.
    
Figure~\ref{fig:energyscore} displays the results of our foray looking into this 
    notion of an optimal starting position, by examining the rise in energy score 
    calculated from a posterior predictive sample from the fitted model against the target 
    sample, over a baseline energy score calculated using another random sample from the same 
    generating distribution.  We interpret this value as the \emph{closer} the curve is to 0, 
    the higher the fidelity with which the model recovers the generating distribution.  
    We investigate this recovery under a variety of conditions increasing the number of dimensions,
    as well as the number of mixture components in the generating distribution. With this simulation
    study, we see that
    a pure MCMC approach achieves the best model fidelity, but \emph{VB Pregamed 2}, using 
    the abridged MCMC sampler to set a starting value for both the centering distribution 
    and cluster weights, achieves a model fidelity in terms of energy score that is very
    nearly indistinguishable from the MCMC model, at least in the scale for which we can test.
    Note, as the dimensionality of the problem grows, energy score becomes less able to assess 
    model fidelity, as the distance between each observation or replication will approach a 
    constant value.\makenote{(needs citation--this is a result from high-dimensional knn)}.

\subsection{Sea, Lake, and Overland Surge from Hurricanes (SLOSH)}
As a basis for our analysis of the extremal dependence structure, we use the aforementioned
    SLOSH dataset, which simulates the storm surge resulting from hurricanes over a wide
    grid.  Our interest is specifically in describing and exploiting the dependence structure 
    of extremes between specific locations.  As such, rather than the entire grid, 
    it makes sense to consider data pertaining specifically to those locations. 
    If one is performing contingency planning in preparation for the storm season, 
    it would be helpful to know the probability of such services being rendered 
    inoperable simultaneously, precisely when they are needed most.  If one is 
    seeking sites for a new service provider, it would be helpful understand the 
    likelihood of a proposed location being rendered inoperable in the same 
    manner.  For these reasons we subset the data to grid cells in the 
    vicinity of such locations of interest.  We gather these locations are from 
    the 2023 US Census Bureau's \emph{TIGER} database \needcite and specifically 
    the point and landmark file.  If a grid cell falls within 70 meters of a location,
    then the grid cell is identified with the location.  If multiple grid cells
    are within 70 meters, then the nearer one is identified with the location.
    Additionally, we select locations, or grid cells, that have experienced at
    at least some inundation in some $q$ proportion of storm simulations.  That is,
    such that $b_{qs} > 0$, for all $s$ of interest.  This restriction arises
    as a consequence of fitting the parameters for the marginal generalized Pareto
    distributions: if the threshold for excesses $b$ is not set above the minimum observed
    value in the dataset, MLE estimation of the other marginal GP parameters will suffer.
    Here we suffer another tradeoff, the implications of which are explored in 
    Figure~\ref{fig:thresholdselection}.  Setting a higher threshold allows more 
    sites to be included in the analysis, but in turn reduces the number of storm 
    simulations which exceed the threshold in at least one dimension, which in turn 
    reduces the amount of information available by which we can estimate the 
    dependence structure.  We create several \emph{slices} of the SLOSH data,
    summarized in Table~\ref{tab:datasets}.

\begin{figure}[ht]
    \centering
    \caption{Trade-offs in threshold specification:
    (Left) Proportion of sites with threshold $b_{qs} > 0$ versus $(1 - q)$; 
    (Right) Proportion of storms surviving thresholding, $\text{P}(\bm{W}_i \not< \bm{b}_q)$ versus $(1 - q)$.
    \label{fig:thresholdselection}}
    \includegraphics[width=0.9\linewidth]{plots/explore_threshold}
\end{figure}

\begin{table}[htb]
    \centering
    \caption{Slices of SLOSH for analysis.  Storms specifies the number of storms that survive thresholding,
    of the total \num{4000} storms in the sample.  The probability gives that value numerically. Sites 
    identifies the number of locations included in the slice. Each subsequent slice is a subset of the  
    preceding slice.\label{tab:datasets}}
    \include{./tables/datadesc}
\end{table}

\begin{figure}[htb]
    \centering
    \caption{Bounds of storm approach angles.\label{fig:slosh_bounds}}
    \includegraphics{plots/slosh_bounds}
\end{figure}

As previously stated, we require the marginal threshold for excesses to be above 0.  We choose
    to implement this restriction at the 90th percentile, which defines the \emph{Threshold} slice.
    \makenote{insert data descriptions}.  Figure~\ref{fig:slosh_bounds} shows the boundaries of 
    storm approach angles in the SLOSH data---observe that this rather restricted region, 
    in comparison to the extent of the data displayed in Figure~\ref{fig:slosh1run},
    somewhat limits the applicability of our methods of analysis in regions outside
    of Delaware bay and its surrounding area.  For this reason, we restrict our analysis
    to this region, creating the \emph{Delaware Bay} slice.  Further refining analysis to locations 
    of particular interest, we use the feature class codes of the locations to isolate location types
    \makenote{remove the most common feature classes} of interest.  These form the \emph{Restricted} slice.
    Finally, we isolate the data to a few locations of critical interest, as well as a selection of
    locations around the bay to inform conditional analysis.  These form the \emph{Critical} slice.
    The relative sizes of these inferential tasks is detailed in Table~\ref{tab:datasets}.  Having
    a variety gives us the opportunity to discuss relative strengths of particular types of analysis.

\begin{figure}[htb]
    \centering 
    \caption{Histograms of characteristics for simulations that survived thresholding in \emph{Threshold}.
        Sea level rise in mm, approach angle in degrees, approach speed in Kph, minimum pressure in mBar,
        latitude in decimal degrees. \makenote{Make this a pairs plot?}
        \label{fig:thetahistogram}}
    \includegraphics[width=0.9\linewidth]{plots/threshold_histogram}
\end{figure}

Figure~\ref{fig:thetahistogram} shows the marginal histograms of storm parameters, for 
    storms which exhibited extreme behavior in the \emph{Threshold} slice.  Storm parameters 
    in the originating data were sampled via Latin hypercube, so would appear marginally
    uniform.  The difference between marginal uniform, and the observed densities provides
    some indication of what characteristics are necessary for a storm to exceed the threshold.
    Imprimis, for sea level rise it is readily apparent that a higher sea level will
    make it easier for a storm to inundate larger swaths of land, or inundate locations to a
    greater degree.  So we expect and, in fact, see a higher proportion of storms exceeding
    the threshold, for a higher sea level rise.  Similarly, a lower minimum pressure in the storm's
    eye corresponds to a more powerful storm.  This bears out, as a lower minimum pressure has
    a higher probability of exceeding the threshold.  The relationship to approach speed is 
    interesting in that it is nearly linear.  Perhaps, the mechanism there lies in that a higher
    approach speed indicates more power behind the storm.  The spike in approach angle past 360
    degrees is interesting as well. 360 degrees indicates due North, thus approach angles 
    beyond 360 degrees indicate the storm is heading slightly northeast. As these approaches are
    on the eastern seaboard, this means a shallower approach angle relative to the
    land---perhaps offering a given storm more time to inundate larger swaths of land.

% EOF 