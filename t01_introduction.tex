%% ABSTRACT %%
% In order to account for the model flexibility required by the multivariate
%     peaks-over-threshold scenario, a Dirichlet process mixture model using
%     the projection of independent gamma random variables onto the unit 
%     hypersphere under the $\mathcal{L}_p$ norm was developed.  This very
%     quickly presents computational issues, as the computational burden for
%     MCMC inference scales superlinearly with sample size, and will scale
%     either linearly or exponentially depending on choice of centering 
%     distribution. We propose an alternate approach, developing a 
%     variational Bayes approach for model inference.  We apply this model
%     to a dataset of simulations of storm surge; comprised of 5 thousand
%     observations at more than 3 million locations.

\subsection{SLOSH}
Sea, Lake, and Overland Surges from Hurricanes \citep{jelesnianski1992} is a 
    computer model developed by the National Weather Service to simulate storm 
    surge, and its associated inundation caused by hurricanes.   Given storm 
    characteristics, the model takes into account local topology, bathymetry, 
    and surge management devices such as levees, to generate a spatial field of 
    inundation---the maximum observed height of water above ground level 
    (or above normal water level for a data point in a body of water) for the 
    duration of the storm at a location.   These storm characteristics include 
    data pertaining to the eye of the storm when it made landfall---bearing, 
    velocity, and latitude, minimum atmospheric pressure of the storm when it 
    made landfall, and projections of sea level rise over time.  A 
    \emph{simulation} from the model is a grid containing \num{23119800} 
    elements wih a spatial resolution of \num{0.001} degrees \makenote{(verify)}, 
    covering an area extending from Virginia Beach, Virginia, to Long Island, 
    New York.  We have \num{4000} such simulations, produced by SLOSH from a 
    sample of storm characteristics.

\makenote{Insert graphic of coverage of model}

Inundation can be catastrophic.  Setting aside the potential for loss of life, 
    flooding can impose costly damage to property: inundation of homes and 
    businesses destroys posessions, and damages buildings through saturating 
    walls and eroding foundations., and thus the risk of damage from flooding 
    provides incentive for flood insurance. \makenote{expand}\needcite  
    Flooding can damage vehicles, and a single storm can force insurance 
    companies to declare large quantities of vehicles as total losses. 
    \makenote{expand}\needcite.  Flooding damages agriculture: beyond 
    destruction of currently growing or stored crops, or the drowning of 
    livestock, inundation by storm surge results in the ground absorbing salt, 
    destroying production capacity of the field until abatement. Flooding can
    damage infrastucture: flooded roads can be washed out or have their 
    foundation damaged, flooded sewers and sewage treatment plants can release 
    their contents above ground imposing additional environmental costs.  
    Flooded power infrastructure, such as transformers can short out causing 
    additional damage. \citep{hutchings2021}.  Inundation can impose additional 
    burdens in the moment:  inundation negatively affects the quality of 
    emergency services, such as a hospital being rendered unable to intake 
    patients.  Sufficient flooding may even render a provider entirely out of 
    commission. 

It makes sense to consider

\subsection{Extremes}



\subsection{Projected Gamma}






The projected gamma based model is not an inherently spatial model.  As such, 
    rather than holding the entire grid in memory, it makes sense to consider 
    data pertaining only to landmarks of interest.    If one is performing 
    contingency planning in preparation for the storm season, it would be 
    helpful to know the probability of such services being rendered inoperable 
    simultaneously, precisely when they are needed most.  If one is seeking 
    sites for a new service provider, it would be helpful understand the 
    likelihood of a proposed location being rendered inoperable in the same 
    manner.  For these reasons, and considering the computational complexity of 
    a 23 million dimensional model, rather than consider the entire grid of data 
    pertaining to any particular storm, we subset the data to grid cells in the 
    vicinity of such locations of interest.  These locations are gathered from 
    the 2023 US Census Bureau's \emph{TIGER} database \needcite and specifically 
    the point and landmark file, and only grid cells that experienced \emph{some} 
    inundation in greater than 2 \makenote{(subject to change)} percent of storm 
    simulations.   This last requirement arises as consequence of fitting the 
    generalized Pareto: the threshold for excesses $b$ needs to be set above the 
    minimum observed value $(0)$ of the dataset.  
    \makenote{(idea: zero-inflated projected gamma?)}  We identify grid cells in 
    the vicinity of such locations by establishing a buffer 

\subsection{Variational methods: a brief overview}
Let $\bm{y}$ be the observed data, and $\bm{\theta}$ be an unobserved set of 
    parameters governing the distribution of the observed data.  We term 
    $f(\bm{y}\mid\bm{\theta})$ the likelihood.  The goal of inference in 
    general is to obtain information about $\theta$, using information 
    obtained from $\bm{y}$. Bayesian inference in particular places a prior
    distribution on $\theta$, $f(\bm{\theta})$, and from this we can obtain a
    posterior distribution on $\bm{\theta}$ that incorporates both information
    from the likelihood as well as the prior.  That is,
    \[
        f(\bm{\theta}\mid\bm{y}) = 
            \frac{f(\bm{y}\mid\bm{\theta})f(\bm{\theta})}{f(\bm{y})},
    \]
    where the demoninator $f(\bm{y})$ is obtained as
    \[
    f(\bm{y}) = \int_{\bm{\theta}}f(\bm{y}\mid\bm{\theta})f(\bm{\theta})d\bm{\theta}.
    \]

When a given model is sufficiently complex that the above integration is no
    longer feasible analytically, or if the posterior cannot be described in
    closed form\makenote{when the posterior is not tractible...}, we frequently 
    turn to sampling--based classes of methods such as 
    \emph{Markov chain Monte Carlo} (MCMC)\needcite. There are myriad approaches 
    to MCMC, \makenote{expand?}, but central to all sample
    based approaches is the stochastic nature of model fitting, and the 
    necessity of samples for posterior inference.  Sampling methods can be 
    tempermental, requiring many numbers of iterations to reach convergence.
    Samples take time to draw, and memory to store; but for sufficiently large 
    problems, both can be in short supply.

% One class of methods for model fitting that side-steps this dependence on 
%     samples of parameters is \emph{expectation maximization} (EM)\needcite.
%     In Bayesian inference, the goal of EM is to find the set of parameters
%     that maximizes posterior density.  That is, 
%     \[
%         \hat{\bm{\theta}}_{\text{MAP}} 
%             = \argmax\limits_{\bm{\theta}} f(\bm{\theta}\mid \bm{y}).
%     \]
%     The output of this approach is a point estimate.  It provides no information
%     about the variability of $\bm{\theta}$, and if $f(\bm{\theta}\mid\bm{y})$ is
%     multimodal, then the resulting point estimate is highly dependent on
%     starting position.

% Another class of methods that side--steps the need for samples, and attempts to
%     include information regarding the variability of $\bm{\theta}$ necessitates
%     specification of a family of \emph{surrogate posteriors}.  Model fitting
%     involves finding the surrogate posterior $q(\bm{\theta})$ that is 
%     \emph{closest} to the true posterior.\makenote{this paragraph is terrible}
%     T

One class of methods that shows promise in avoiding the computational and storage
    burdens of sampling methods is variational inference.\needcite  Based on variational
    calculus, this approach attempts to approximate the true posterior distribution
    $f(\bm{\theta}\mid\bm{y})$ using a tractible \emph{surrogate posterior} 
    $q(\bm{\theta})$.  Model fitting under this class involves specification of the
    family of surrogate densities $\mathcal{Q}$, then selecting that density 
    $q^*$ which minimizes the \emph{Kullbeck-Liebler} (KL) divergence between 
    the surrogate and true posterior.  That is,
    \[
        % q^*(\bm{\theta}) = \argmin_{q\in\mathcal{Q}}\left\lbrace
        % \text{KL}\left(q(\bm{\theta})||f(\bm{\theta}\mid\bm{y})\right) 
        % \coloneqq
        % \int_{\bm{\theta}}q(\bm{\theta})
        %     \frac{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
        %     d\bm{\theta}
        % \right\rbrace.
        q^*(\bm{\theta}) = \argmin_{q\in\mathcal{Q}}\left\lbrace
        \text{KL}\left(q(\bm{\theta})||f(\bm{\theta}\mid\bm{y})\right) 
        \coloneqq
        \text{E}_{q}\left[\log\left(
        \frac{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
        \right)\right]
        \right\rbrace.
    \]
    Using KL divergence as an analogue to distance, we are attempting to find
    the \emph{closest} tractible surrogate posterior $q$ within the family $\mathcal{Q}$ 
    to the true posterior.
    As has been explained however, we do not necessarily have $f(\bm{\theta}\mid\bm{y})$
    directly.  Let $f(\bm{y},\bm{\theta}) = f(\bm{y}\mid\bm{\theta})f(\bm{\theta})$.
    Then we reformulate the KL divergence into
    \[
        \begin{aligned}
        \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
            &=\text{E}_{q}\left[\log\left(
                \frac{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
                \right)\right]\\
            &= \text{E}_{q}\left[
                \frac{q(\bm{\theta})f(\bm{y})}{f(\bm{y},\bm{\theta})}
                \right]\\
            &= \text{E}_{q}\left[
                \log q(\bm{\theta}) - \log f(\bm{y},\bm{\theta}) 
                + \log f(\bm{y})
                \right]\\
            &= \text{E}_{q}\left[
                \log q(\bm{\theta}) - \log f(\bm{y},\bm{\theta})\right] + 
                    \text{E}_{q}\left[\log f(\bm{y})\right].\\
            &= \text{E}_{q}\left[
                \log q(\bm{\theta}) - \log f(\bm{y},\bm{\theta})\right] + 
                   \log f(\bm{y}).
        \end{aligned}
    \]
    Note that the second term, the \emph{evidence}, is constant with respect to 
    $\bm{\theta}$.  With a bit of rearrangement,
    \[
        \begin{aligned}
        \log f(\bm{y}) 
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
                - \text{E}_q\left[
                \log q(\bm{\theta}) - \log f(\bm{y},\bm{\theta})
                \right]\\
            &= \text{KL}\infdiv{q(\bm{\theta})}{f(\bm{\theta}\mid\bm{y})}
                + \mathcal{L}(\bm{\theta})
        \end{aligned}
    \]
    Thus maximizing $\mathcal{L}(\bm{\theta})$ is equvivalent to minimizing the 
    KL divergence. In this form, as KL divergence is a positive quantity, it is
    readily apparent that $\mathcal{L}(\bm{\theta})$ establishes a lower bound
    on the evidence.  As such, the variational literature has christened it the
    \emph{evidence lower bound}, frequently abbreviated to \emph{ELBO}.
    
\subsection{Optimizing the ELBO: Automatic Differentiation}

% EOF 