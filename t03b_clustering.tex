\begin{comment}
    \begin{itemize}
        \item Description of storm parameter clustering problem
        \item Introduction of variational inference
        \begin{itemize}
            \item Describe variational distribution
            \item Necessity (only model using t90 dataset)
            \item Relative performance (computational speed)
        \end{itemize}
        \item Describe clustering methodology
        \item Emergent clusters from both t90 and all--but--road--features in input space.
        \item What does this mean?
    \end{itemize}
\end{comment}

One potential application leveraging use of a Bayesian non-parametric prior is 
    exploiting the clustering inherent to the method. Recall $\delta_i$ is the 
    cluster identifier for observation $i$.  Its sampling is made explicit in
    the MCMC model, though it is integrated out in our variational model. In
    posterior analysis, given $\bm{\alpha}$, and $\bm{\pi}$, where 
    $\pi_j = \nu_j\prod_{k = 1}^{j-1}(1 - \nu_k)$, a probability of cluster 
    assignment can be sampled as
    \begin{equation}
        \label{eqn:clusterprob}
        \text{P}\left(\delta_i = j\mid\bm{\alpha},\bm{\nu},\bm{y}_i\right) 
            = \frac{\pi_j\mathcal{PG}(\bm{y}_i\mid\bm{\alpha}_j,\bm{1})}{
            \sum_{k = 1}^J \pi_j\mathcal{PG}(\bm{y}_i\mid\bm{\alpha}_k,\bm{1})}.
    \end{equation}
    This approach relies on samples of $\bm{\alpha},\bm{\nu}$ from the fitted
    model to generate \emph{loose} clusters for which there is no inherent 
    labelling.  This presents a problem, as interpreting clusters first requires
    \emph{labelling}, or a fixed assignment of observations to clusters.

The variational implementation avoids a label-switching issue inherent to 
    MCMC sampling.  A label-switch between clusters $a$ and $b$ has occured when
    the parameters of clusters $a$ and $b$ have nearly swapped, such that the
    bulk of observations formerly in cluster $a$ are now in cluster $b$, and 
    visa-versa.  This issue does not tend to follow to a variational 
    implementation, as post-fitting, the distributions of cluster parameters are 
    fixed.

With that being the case, a simple option for cluster labelling might be to take
    \[
        \bm{\alpha}_j^* = \frac{1}{S}\sum_{s = 1}^S\bm{\alpha}_{js}\hspace{2cm}
        \bm{\nu}_j^* = \frac{1}{S}\sum_{s = 1}^S \nu_{js},
    \]
    the posterior mean of cluster parameters and cluster weights, then set
    $\delta_i^{*} = \argmax P(\delta_i = j\mid\bm{\alpha}^*,\bm{\nu}^*,\bm{y}_i)$
    as defined in Equation~\eqref{eqn:clusterprob}. We refer to cluster labels
    being assigned in this fashion as \emph{posterior mean clustering}.

Alternative to posterior mean clustering, we could instead take samples of 
    $\bm{\pi}$, $\bm{\alpha}$, then compute 
    $\delta_{is}\mid\bm{\pi}_s,\bm{\alpha}_s$
    as per Equation~\eqref{eqn:clusterprob}.  Then set 
    $\delta_i^{*} = \argmax \mathbbm{1}_{\delta_{is} = j}$.
    We refer to cluster labels being assigned in this fashion as
    \emph{posterior assignment clustering}.  We find, in practice, this method
    results in largely the same label sets as posterior mean clustering.


% EOF